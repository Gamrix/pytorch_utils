diff --git a/c10/core/TensorImpl.h b/c10/core/TensorImpl.h
index ad68332cc9..d3363addbe 100644
--- a/c10/core/TensorImpl.h
+++ b/c10/core/TensorImpl.h
@@ -577,6 +577,22 @@ struct C10_API TensorImpl : public c10::intrusive_ptr_target {
     return strides_default();
   }
 
+  /*
+   * Returns the optimized SizeAndStride object for performance critical copies
+   * of tensor sizes and strides.
+   */
+  impl::SizesAndStrides sizes_and_strides() const {
+    if (C10_UNLIKELY(
+            sizes_strides_policy_ >=
+            static_cast<uint8_t>(SizesStridesPolicy::CustomStrides))) {
+      auto ss = impl::SizesAndStrides();
+      ss.set_sizes(sizes_custom());
+      ss.set_strides(SymIntArrayRef::fromIntArrayRef(strides_custom()));
+      return ss;
+    }
+    return sizes_and_strides_;
+  }
+
   /**
    * Return the size of a tensor at some dimension, wrapping the dimension if
    * necessary.
diff --git a/test/test_profiler.py b/test/test_profiler.py
index b926f276de..444f025f3d 100644
--- a/test/test_profiler.py
+++ b/test/test_profiler.py
@@ -1144,9 +1144,9 @@ class TestTorchTidyProfiler(TestCase):
             node.children[0].children[0].extra_fields,
             torch._C._autograd._ExtraFields_Allocation)
 
-    def test_tensor_sizes(self):
-        x = torch.ones(10, 10)
-        y = torch.ones(1, 10)
+    def test_tensor_sizes_strides(self):
+        x = torch.ones(10, 10).as_strided([4, 4], [12, 3])
+        y = torch.ones(4, 1)
 
         with profile(with_stack=True, profile_memory=True, record_shapes=True) as p:
             _ = x + y
@@ -1160,8 +1160,34 @@ class TestTorchTidyProfiler(TestCase):
             torch._C._autograd._ExtraFields_TorchOp)
 
         # The alpha scalar has a [] size
-        self.assertEqual(node.extra_fields.inputs.shapes, [[10, 10], [1, 10], []])
-        self.assertEqual(node.extra_fields.inputs.dtypes, ['float', 'float', 'Scalar'])
+        input_info = node.extra_fields.inputs
+        self.assertEqual(input_info.dtypes, ['float', 'float', 'Scalar'])
+
+        shape_info = [x.shape if x else None for x in input_info.tensor_metadata]
+        stride_info = [x.stride if x else None for x in input_info.tensor_metadata]
+        layout_info = [x.layout if x else None for x in input_info.tensor_metadata]
+
+        self.assertEqual(layout_info, [torch.strided, torch.strided, None])
+        self.assertEqual(shape_info, [[4, 4], [4, 1], None])
+        self.assertEqual(stride_info, [[12, 3], [1, 1], None])
+
+    def test_scalar_ins(self):
+        x = torch.ones(5, 5)
+        alpha = 0.9
+
+        with profile(with_stack=True, profile_memory=True, record_shapes=True) as p:
+            _ = torch.add(x, 9.1, alpha=alpha)
+
+        nodes = p.profiler.kineto_results.experimental_event_tree()
+        node = find_node_with_name(nodes, "aten::add")
+        self.assertIsNotNone(node)
+
+        # The second argument to the add gets promotoed to a zerodim Tensor
+        input_info = node.extra_fields.inputs
+        self.assertEqual(input_info.dtypes, ['float', 'double', 'Scalar'])
+        shape_info = [x.shape if x else None for x in input_info.tensor_metadata]
+        self.assertEqual(shape_info, [[5, 5], [], None])
+        self.assertEqual(input_info.ivalues, [None, None, alpha])
 
 
 @dataclass(frozen=True)
diff --git a/torch/csrc/autograd/init.cpp b/torch/csrc/autograd/init.cpp
index d4de91cf3c..132b220ddf 100644
--- a/torch/csrc/autograd/init.cpp
+++ b/torch/csrc/autograd/init.cpp
@@ -270,14 +270,46 @@ PyObject* THPAutograd_initExtension(PyObject* _unused, PyObject* unused) {
 
   {
     using torch::profiler::impl::Result;
+
     py::class_<ExtraFields<EventType::TorchOp>>(m, "_ExtraFields_TorchOp")
         .def_readonly("inputs", &ExtraFields<EventType::TorchOp>::inputs_);
 
     py::class_<Inputs>(m, "_Inputs")
-        .def_readonly("shapes", &Inputs::shapes_)
+        .def_property_readonly(
+            "ivalues",
+            [](const Inputs& inputs) {
+              py::list list;
+              for (auto& v : inputs.ivalues_) {
+                list.append(torch::jit::toPyObject(v));
+              }
+              return list;
+            })
+        .def_readonly("tensor_metadata", &Inputs::tensor_metadata_)
         .def_readonly("dtypes", &Inputs::dtypes_);
 
-    py::class_<ExtraFields<EventType::Backend>>(m, "_ExtraFields_Backend");
+    py::class_<TensorMetadata>(m, "_TensorMetadata")
+        .def_readonly("layout", &TensorMetadata::layout_)
+        .def_property_readonly(
+            "shape",
+            [](const TensorMetadata& metadata) {
+              py::list arg_shape_list;
+              for (auto size : metadata.sizes_and_strides_.sizes_arrayref()) {
+                // TODO: Someday support Symbolic Ints
+                arg_shape_list.append(size.expect_int());
+              }
+              return arg_shape_list;
+            })
+        .def_property_readonly(
+            "stride",
+            [](const TensorMetadata& metadata) {
+              py::list arg_stride_list;
+              for (auto stride : metadata.sizes_and_strides_.strides_arrayref()) {
+                // TODO: Someday support Symbolic Ints
+                arg_stride_list.append(stride.expect_int());
+              }
+              return arg_stride_list;
+            });
+    py::class_<ExtraFields<EventType::Backend>>( m, "_ExtraFields_Backend");
     py::class_<ExtraFields<EventType::Allocation>>(
         m, "_ExtraFields_Allocation");
     py::class_<ExtraFields<EventType::PyCall>>(m, "_ExtraFields_PyCall");
diff --git a/torch/csrc/autograd/profiler_kineto.cpp b/torch/csrc/autograd/profiler_kineto.cpp
index f7305a0740..649855d0c8 100644
--- a/torch/csrc/autograd/profiler_kineto.cpp
+++ b/torch/csrc/autograd/profiler_kineto.cpp
@@ -83,10 +83,21 @@ struct EventFieldsVisitor {
         .debugHandle(op_event.debug_handle_)
         .setAsync(op_event.is_async_);
 
-    auto& shapes = op_event.inputs_.shapes_;
-    if (!shapes.empty()) {
-      kineto_event_.get().shapes(shapes);
+    auto& metadata = op_event.inputs_.tensor_metadata_;
+    if (!metadata.empty()) {
+      std::vector<std::vector<int64_t>> shapes;
+      for(auto& tensor_metadata : metadata) {
+        shapes.emplace_back();
+        if(tensor_metadata){
+          auto& last_shape = shapes.back();
+          for (auto s: tensor_metadata->sizes_and_strides_.sizes_arrayref()){
+            // TODO: Someday support Symbolic Ints
+            last_shape.emplace_back(s.expect_int());
+          }
+        }
+      }
       annotations_.emplace_back("Input Dims", shapesToStr(shapes));
+      kineto_event_.get().shapes(shapes);
     }
 
     auto& dtypes = op_event.inputs_.dtypes_;
diff --git a/torch/csrc/profiler/collection.cpp b/torch/csrc/profiler/collection.cpp
index 67ceb90820..ebef06542f 100644
--- a/torch/csrc/profiler/collection.cpp
+++ b/torch/csrc/profiler/collection.cpp
@@ -21,6 +21,8 @@ void InputOutputEncoder::push(c10::ArrayRef<const c10::IValue> values) {
       push(value.toTensor());
     } else if (value.isScalar()) {
       tags_.emplace_back(Tag::Scalar);
+      // Scalars are small enough to store as IValues
+      ivalues_.emplace_back(value);
     } else if (value.isTensorList()) {
       tags_.emplace_back(Tag::TensorListBegin);
       // TODO: Skip TensorList for now.
@@ -35,43 +37,40 @@ void InputOutputEncoder::push(c10::ArrayRef<const c10::IValue> values) {
 void InputOutputEncoder::push(const at::Tensor& t) {
   if (t.defined()) {
     tags_.emplace_back(Tag::Tensor);
-    const auto& sizes = t.sizes();
-    const auto dim = sizes.size();
+    const auto dim = t.dim();
     TORCH_CHECK(
         dim <= std::numeric_limits<uint32_t>::max(),
         "Cannot profile Tensors of size > uint32 max. Got dim: ",
         dim);
 
     tensor_metadata_.emplace_back(
-        /*ptr_=*/(void*)t.unsafeGetTensorImpl(),
+        /*ptr_*/ t.unsafeGetTensorImpl(),
         /*dtype_=*/t.scalar_type(),
-        /*dim_=*/(uint32_t)dim);
-
-    for (const auto i : sizes) {
-      tensor_sizes_.emplace_back(i);
-    }
+        /*dim_=*/(uint32_t)dim,
+        /*sizes_and_strides_*/ t.unsafeGetTensorImpl()->sizes_and_strides(),
+        /*layout_=*/t.layout());
   } else {
     tags_.emplace_back(Tag::UndefinedTensor);
   }
 }
 
+// /*UNSAFE_tensor_impl_ptr_*/ t.unsafeGetTensorImpl(),
+// /*UNSAFE_storage_impl_ptr_*/ t.storage().unsafeGetStorageImpl(),
+
 // This is a custom-iterator-like getter to obtain input shapes and dtypes.
 auto InputOutputEncoder::getNextShapesAndDtypes() {
   return [this,
           tag_it = tags_.begin(),
           tensor_metadata_it = tensor_metadata_.begin(),
-          tensor_size_it = tensor_sizes_.begin()]() mutable {
+          ivals_it = ivalues_.begin()]() mutable {
     struct Inputs out;
     bool terminate = false;
     while (!terminate && tag_it != tags_.end()) {
-      out.shapes_.emplace_back();
       switch (*tag_it) {
         case Tag::Tensor: {
+          out.ivalues_.emplace_back();
           const auto& md = *tensor_metadata_it++;
-          for (const auto _ : c10::irange(md.dim_)) {
-            (void)_; // Suppress unused variable warning
-            out.shapes_.back().push_back(*tensor_size_it++);
-          }
+          out.tensor_metadata_.emplace_back(md);
           out.dtypes_.emplace_back(scalarTypeToTypeMeta(md.dtype_).name());
         } break;
 
@@ -80,20 +79,25 @@ auto InputOutputEncoder::getNextShapesAndDtypes() {
             // TODO: Skip TensorLists for now.
           }
           out.dtypes_.emplace_back("TensorList");
+          out.ivalues_.emplace_back();
+          out.tensor_metadata_.emplace_back();
           break;
 
         case Tag::Scalar:
           out.dtypes_.emplace_back("Scalar");
+          out.ivalues_.emplace_back(*ivals_it++);
+          out.tensor_metadata_.emplace_back();
           break;
 
         case Tag::UndefinedTensor:
         case Tag::Other:
+          out.ivalues_.emplace_back();
           out.dtypes_.emplace_back();
+          out.tensor_metadata_.emplace_back();
           break;
 
         case Tag::TERMINATOR:
           // This marks the end of this op.
-          out.shapes_.pop_back();
           terminate = true;
           break;
 
@@ -109,7 +113,7 @@ auto InputOutputEncoder::getNextShapesAndDtypes() {
 void InputOutputEncoder::clear() {
   tags_.clear();
   tensor_metadata_.clear();
-  tensor_sizes_.clear();
+  ivalues_.clear();
 }
 
 namespace {
@@ -426,6 +430,67 @@ struct ResultGreater {
   }
 };
 
+/*
+void calculate_unique_tensor_ids(std::vector<result_ptr_t>& sorted_results) {
+  // Go through the result vectors
+  // For Torch Ops, check if (TensorImpl*, StorageImpl*) is already in the map
+  // For allocator events with negative allocation sizes,
+
+  // Do I need to track both TensorImpl and StorageImpl?
+
+  struct UniqueTensorInfo {
+    c10::StorageImpl* storage_impl_;
+    size_t unique_tensor_id_;
+  };
+
+  size_t cur_unique_tensor_id = 1;
+
+  std::unordered_set<void*> valid_storage_impls;
+  std::unordered_map<void*, UniqueTensorInfo> unique_tensor_map;
+
+  for (auto& res : sorted_results) {
+    if (auto torch_op =
+            c10::get_if<ExtraFields<EventType::TorchOp>>(&res->extra_fields_)) {
+      auto& metadata_vec = torch_op->inputs_.tensor_metadata_;
+      for (auto& metadata : metadata_vec) {
+        if (!metadata) {
+          continue;
+        }
+        auto tensor_impl = metadata->UNSAFE_tensor_impl_ptr_;
+        auto storage_impl = metadata->UNSAFE_storage_impl_ptr_;
+        if (!tensor_impl || !storage_impl) {
+          continue;
+        }
+        if (valid_storage_impls.find(storage_impl) ==
+            valid_storage_impls.end()) {
+          valid_storage_impls.insert(storage_impl);
+          cur_unique_tensor_id++;
+          unique_tensor_map[tensor_impl] = {storage_impl, cur_unique_tensor_id};
+          metadata->unique_tensor_id_ = cur_unique_tensor_id;
+          continue;
+        }
+        auto it = unique_tensor_map.find(tensor_impl);
+        if (it == unique_tensor_map.end()) {
+          cur_unique_tensor_id++;
+          unique_tensor_map[tensor_impl] = {storage_impl, cur_unique_tensor_id};
+          metadata->unique_tensor_id_ = cur_unique_tensor_id;
+        } else {
+          metadata->unique_tensor_id_ = it->second.unique_tensor_id_;
+        }
+      }
+    } else if (
+        auto alloc_op = c10::get_if<ExtraFields<EventType::Allocation>>(
+            &res->extra_fields_)) {
+      if (alloc_op->alloc_size_ < 0) {
+        // For deallocations, clear relevant pointers
+        valid_storage_impls.erase(alloc_op->ptr_);
+        unique_tensor_map.erase(alloc_op->ptr_);
+      }
+    }
+  }
+}
+*/
+
 void build_tree(std::vector<std::shared_ptr<Result>>& events) {
   set_autograd_evaluate(events);
   std::stable_sort(
@@ -433,6 +498,9 @@ void build_tree(std::vector<std::shared_ptr<Result>>& events) {
         return a->start_time_ns_ < b->start_time_ns_;
       });
 
+  // Needs both TorchOp and Allocation events in order.
+  //calculate_unique_tensor_ids(events);
+
   using op_fields = ExtraFields<EventType::TorchOp>;
   ska::flat_hash_map<uint64_t, std::shared_ptr<Result>> stacks;
   std::priority_queue<result_ptr_t, std::vector<result_ptr_t>, ResultGreater>
diff --git a/torch/csrc/profiler/collection.h b/torch/csrc/profiler/collection.h
index c070d41c31..229731f72d 100644
--- a/torch/csrc/profiler/collection.h
+++ b/torch/csrc/profiler/collection.h
@@ -43,9 +43,22 @@ struct TorchOpBasicFields {
   // Set in the exit callback.
   uint64_t end_tid_{0};
 };
+//c10::TensorImpl* UNSAFE_tensor_impl_ptr_;
+//c10::StorageImpl* UNSAFE_storage_impl_ptr_;
+
+
+struct TensorMetadata {
+  void* ptr_;
+  c10::ScalarType dtype_;
+  uint32_t dim_;
+  c10::impl::SizesAndStrides sizes_and_strides_;
+  c10::Layout layout_;
+  size_t unique_tensor_id_;
+};
 
 struct Inputs {
-  std::vector<std::vector<int64_t>> shapes_;
+  std::vector<c10::optional<TensorMetadata>> tensor_metadata_;
+  std::vector<c10::IValue> ivalues_;
   std::vector<std::string> dtypes_;
 };
 
@@ -262,11 +275,6 @@ class InputOutputEncoder final {
     TERMINATOR
   };
 
-  struct TensorMetadata {
-    void* ptr_;
-    c10::ScalarType dtype_;
-    uint32_t dim_;
-  };
 
   void push(const at::Tensor& t);
 
@@ -274,6 +282,8 @@ class InputOutputEncoder final {
   AppendOnlyList<TensorMetadata, IO_ENCODER_DEFAULT_BLOCK_SIZE>
       tensor_metadata_;
   AppendOnlyList<int64_t, IO_ENCODER_DEFAULT_BLOCK_SIZE> tensor_sizes_;
+  AppendOnlyList<int64_t, IO_ENCODER_DEFAULT_BLOCK_SIZE> tensor_strides_;
+  AppendOnlyList<c10::IValue, IO_ENCODER_DEFAULT_BLOCK_SIZE> ivalues_;
 };
 
 class RecordQueue;
diff --git a/torch/csrc/profiler/containers.h b/torch/csrc/profiler/containers.h
index 78fab227f6..ebbb7f1dfa 100644
--- a/torch/csrc/profiler/containers.h
+++ b/torch/csrc/profiler/containers.h
@@ -9,6 +9,7 @@
 #include <vector>
 
 #include <c10/macros/Macros.h>
+#include <c10/util/ArrayRef.h>
 #include <c10/util/Exception.h>
 
 namespace torch {
